# –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø
My_AI —ç—Ç–æ –Ω–µ–±–æ–ª—å—à–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ (—Å–∫–æ—Ä–µ–µ –¥–∞–∂–µ –º–∏–∫—Ä–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–µ–Ω—å–∫–æ–≥–æ –ò–ò. –Ø –µ—ë –Ω–∞–ø–∏—Å–∞–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –Ω—É–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ numpy, –∞ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —è –±—Ä–∞–ª –¢–û–õ–¨–ö–û –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã. –¢–∞–∫–∂–µ —è –∏—Å—Ç–∞–≤–∏–ª –∫—É—á—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –∫–æ–¥—É, —á—Ç–æ–±—ã –≤—ã –º–æ–≥–ª–∏ —Å–∞–º–∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –∫–∞–∫ –∑–¥–µ—Å—å –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –ø–æ–∫–∞–ø–∞—Ç—å—Å—è –≤ –∫–æ–¥–µ (–º–æ–∂–µ—Ç –¥–∞–∂–µ —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ —É–∑–Ω–∞–µ—Ç–µ)

# –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É: üëâ
# –°—É–ø–µ—Ä –∫—Ä–∞—Ç–∫–æ:
```python
from My_AI import AI_ensemble, AI

# –°–æ–∑–¥–∞—ë–º –ò–ò
ai = AI(architecture=[10, 50, 50, 1],
        add_bias_neuron=True,
        name="First_AI")
""" –ò–ª–∏ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∞–Ω—Å–∞–º–±–ª—å
ai = AI_ensemble(amount_ais=10, architecture=[10, 50, 50, 1],
                      add_bias_neuron=True,
                      name="First_AI")
"""


# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
ai.alpha = 1e-3
ai.batch_size = 10

ai.what_act_func = ai.kit_act_func.tanh
ai.end_act_func = ai.kit_act_func.tanh
ai.save_dir = "Saves AIs"

# –û–±—É—á–∞–µ–º (–ù–∞–ø—Ä–∏–º–µ—Ä —Ä–∞—Å–ø–æ–∑–Ω–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏)
for image in dataset:
    data = image.tolist()
    answer = [image.what_num]
    ai.learning(data, answer, squared_error=True)

""" –ï—Å—Ç—å —Ç–∞–∫–∂–µ –∏ Q-–±—É—á–µ–Ω–∏–µ (—Ñ—É–Ω–∫—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã —Å–º. –Ω–∏–∂–µ)
actions = ("left", "right", "up", "down")
ai.make_all_for_q_learning(actions, ai.kit_upd_q_table.standart,
                           gamma=0.5, epsilon=0.01, q_alpha=0.1)

state, reward = game.get_state_and_reward()
ai.q_learning(state, reward,
              learning_method=2.5, squared_error=False)
"""
```




# –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏:

#### –ú–æ–∂–µ—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç "My_AI" –∫ —Å–µ–±–µ –≤ –ø—Ä–æ–µ–∫—Ç (–≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è), –∏ –∏–º–ø–æ—Ä–∏—Ç—Ä–æ–≤–∞—Ç—å –æ—Ç —Ç—É–¥–∞ –∫–ª–∞—Å—Å—ã AI –∏–ª–∏ AI_ensemble

### –ö–∞–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ò–ò—à–∫—É:
```python
from My_AI import AI_ensemble, AI

ai = AI()
# –ò–ª–∏
ensemble = AI_ensemble(5) # 5 ‚Äî‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ò–ò—à–µ–∫ –≤ –∞–Ω—Å–∞–º–±–ª–µ
```
> –ê–Ω—Å–∞–º–±–ª—å ‚Äî —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ò–ò –≤ –æ–¥–Ω–æ–π –∫–æ—Ä–æ–±–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ –≤–º–µ—Å—Ç–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç —Ä–µ—à–µ–Ω–∏–µ (–∞–Ω—Å–∞–º–±–ª—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Q-–æ–±—É—á–µ–Ω–∏—è (–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º; –∫–æ–≥–¥–∞ –Ω–µ—Ç—É "–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ" –∏ "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ" –æ—Ç–≤–µ—Ç–∞, –∞ —Ç–æ–ª—å–∫–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –∫–∞–∫–æ–µ-—Ç–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ))

> P.s. –í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –ò–ò –¥–ª—è –∑–º–µ–π–∫–∏ (–≤ —Ñ–∞–π–ª–µ "AI for snake.py")


### ‚Ä¢ –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ò–ò—à–∫—É –Ω–∞–¥–æ —Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤:
```python
ai = AI(architecture=[3, 4, 4, 4, 3],
        add_bias_neuron=True,
        name="First_AI")

""" –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –∞–Ω—Å–∞–º–±–ª—å
ensemble = AI_ensemble(5, architecture=[3, 4, 4, 4, 3],
                       add_bias_neuron=True,
                       name="First_AI")
"""
```
–∏–ª–∏
```python
ai.create_weights([3, 4, 4, 4, 3],
                  add_bias_neuron=True)
ai.name = "First_AI"
# –ò–º—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –∏–º—è, –Ω–æ —Ç–æ–≥–¥–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
# –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å–ª—É—á–∞–π–Ω–æ–µ —á–∏—Å–ª–æ –≤–º–µ—Å—Ç–æ –∏–º–µ–Ω–∏
```
–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ —Å–ª–µ–¥—É—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
<div id="header" align="left">
  <img src="https://i.ibb.co/nbbTLZS/Usage-example.png" width="600"/>
</div>


#### ¬†
### ‚Ä¢ –ì–∞–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
```python
"""–ü—Ä–æ–ø–∏—Å—ã–≤–∞—Ç—å –∏–ª–∏ –∏–∑–º–µ–Ω—è—Ç—å –≤—Å–µ –≥–∞–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ"""

ai.alpha = 1e-2  # –ê–ª—å—Ñ–∞ (—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è)

ai.number_disabled_weights = 0.0  # –ö–∞–∫—É—é –¥–æ–ª—é –≤–µ—Å–æ–≤ –æ—Ç–∫–ª—é—á–∞–µ–º
# (–≠—Ç–æ –Ω–∞–¥–æ —á—Ç–æ–±—ã –Ω–µ –≤–æ–∑–Ω–∏–∫–∞–ª–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)

ai.batch_size = 10  # –°–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤ —É—Å—Ä–µ–¥–Ω—è–µ–º, —á—Ç–æ–±—ã –Ω–∞ –Ω–∏—Ö —É—á–∏—Ç—å—Å—è
# (–£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ (–∏–Ω–æ–≥–¥–∞, –¥–∞–ª–µ–∫–æ –Ω–µ –≤—Å–µ–≥–¥–∞) —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è)

# –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤ (–∫—Ä–∞–π–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ—Å—Ç–∞–≤–∏—Ç—å tanh,
# —Ç.–∫. —Å –Ω–∏–º –ò–ò —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–∞–∑—ã –±—ã—Å—Ç—Ä–µ–µ)
ai.what_act_func = ai.kit_act_func.tanh

# –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ—Å—Ç–∞–≤–∏—Ç—å tanh)
# P.s. end_act_func –º–æ–∂–µ—Ç –∏ –æ—Ç—Å—Ç—É—Ç—Å–≤–æ–≤–∞—Ç—å (—Ç.–µ. –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å None)
ai.end_act_func = ai.kit_act_func.tanh
```


#### ¬†
### ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ:

```python
# –ù–∞ –≤—Ö–æ–¥ –∏ –Ω–∞ –≤—ã—Ö–æ–¥ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞–¥–æ –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–∞–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª
data = [0, 1, 2]   # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
answer = [0, 1, 0] # –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

ai.learning(data, answer,
            squared_error=True)
"""
–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –∫–æ—Å—è–∫–∞—Ö
–∏ –∑–∞–∫—Ä—ã–≤–∞—Ç—å –≥–ª–∞–∑–∞ –Ω–∞ –º–µ–ª–∫–∏–µ –Ω–µ–¥–æ—á—ë—Ç—ã (–Ω–æ –∏–Ω–æ–≥–¥–∞ –ª—É—á—à–µ –µ—ë –æ—Ç–∫–ª—é—á–∞—Ç—å)
"""
```

### Q-–±—É—á–µ–Ω–∏–µ:
> Q-–æ–±—É—á–µ–Ω–∏–µ (–æ–Ω–æ –∂–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º) ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –Ω–µ—Ç—É "–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ" –∏ "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ" –æ—Ç–≤–µ—Ç–∞, –∞ —Ç–æ–ª—å–∫–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –∫–∞–∫–æ–µ-—Ç–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, —Ç.–µ. –Ω–∞ —Å–∫–æ–ª—å–∫–æ –æ–Ω–æ —Ö–æ—Ä–æ—à–µ–µ 

> (0 = –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä, <0 = –ø–ª–æ—Ö–æ–π –≤—ã–±–æ—Ä, >0 = —Ö–æ—Ä–æ—à–∏–π –≤—ã–±–æ—Ä)

```python
# –ò–ò—à–∫–∞ –º–æ–∂–µ—Ç —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞—Ç—å –∫–∞–∫–æ–µ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ –≤–æ–∑–º–∞–∂–Ω—ã—Ö
all_possible_actions = ["left", "right", "up", "down"]

gamma = 0.4     # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç "–¥–æ–≤–µ—Ä–∏—è –æ–ø—ã—Ç—É"
epsilon = 0.15  # –î–æ–ª—è –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π (—á—Ç–æ–±—ã –ò–ò—à–∫–∞ –∏–∑—É—á–∞–ª–∞ –æ–∫—Ä—É–∂–∞—é—â—É—é —Å—Ä–µ–¥—É)
q_alpha = 0.1   # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Q-—Ç–∞–±–ª–∏—Ü—ã (–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –æ–Ω–æ –ø–æ—á—Ç–∏ –Ω–∏ –Ω–∞ —á—Ç–æ –Ω–µ –≤–ª–∏—è–µ—Ç) 

ai.make_all_for_q_learning(all_possible_actions,
                           ai.kit_upd_q_table.standart,
                           gamma=gamma, epsilon=epsilon, q_alpha=q_alpha)


# Also make sure the number of input neurons is equal to the size of the state list
ai_state = [0, 1]  # For example, coordinates

ai.q_learning(ai_state, reward_for_state, learning_method=2.2,
              recce_mode=False, squared_error=True)
"""
recce_mode - if set to True, enable "reconnaissance mode",
i.e. in this mode, the AI does not learn, but only the Q-table is replenished
(and random actions are performed)
P.s. I recommend turning it on before training

Errors can be: regular, quadratic, logarithmic

Regularization can be: quadratic (the more weight, the more punish),
                       penalty   (if weights exceed regularization_value, then we punish)

impulse_coefficient: Momentum factor in Adam optimizer (usually around 0.7 ~ 0.99)


You can choose the most suitable q_table-table update function for you
P.s. The difference between the functions is negligible

learning_method :
1 : As the "correct" answer, the one that is most rewarded is selected
P.s. This is not very good, because other options that bring either the
     same or a little less reward are ignored (and only one "correct" one is selected).
     BUT IT IS WELL SUITABLE WHEN YOU HAVE EXCLUSIVELY ONE CORRECT ANSWER
     IN THE PROBLEM AND THERE CANNOT BE "MORE" AND "LESS" CORRECT

2: Making more useful answers more ‚Äúcorrect‚Äù
(in the fractional part after 2, indicate the degree by which we notice the discrepancy
between the ‚Äúmore‚Äù and ‚Äúless‚Äù correct answers
(for example: 2.345 means a degree of difference of 3.45))

BTW, if your AI learns very badly (or does not learn at all), then look at the Q-table, if there are mostly (> 50%) negative numbers, then in this case you need to reward more and punish less (so that there are more positive numbers)
"""
```
> And if you want to see for yourself what answer the AI gave out, then just pass the input data to this method:  ai.q_start_work(data)


#### ¬†
#### ‚Ä¢ Or then you can create several AIs, and then cross the best of them, using the method:
```python
better_ai_0.genetic_crossing_with(better_ai_1)
```


#### ‚Ä¢ Or you can change (mutate) the AI so that it doesn't stand still or hope that some of mutations turn out to be good

```python
ai.make_mutations(0.05)  # Replacing 5% of all weights with random numbers
```

#### ¬†
### ‚Ä¢ If your input data can take any value and/or vary over a large range, then normalize it with :

```python
# (Better to normalize from 0 to 1 OR -1 to 1)
ai.kit_act_funcs.normalize(data, min_value, max_value)
```


#### ¬†
#### ¬†
#### ¬†
#### ¬†
Good luck
(„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°)„Å•
